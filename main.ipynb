{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92da7064",
   "metadata": {},
   "source": [
    "## HouseHunt Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc385c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "# %pip install undetected-chromedriver selenium beautifulsoup4\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import undetected_chromedriver as uc ## use py 3.11 or lesser for this to work.\n",
    "\n",
    "\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Output file path\n",
    "output_file = 'output.parquet'\n",
    "\n",
    "# Setup Chrome options\n",
    "options = uc.ChromeOptions()\n",
    "options.add_argument('--no-sandbox')\n",
    "driver = uc.Chrome(options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dbafd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## General Functions\n",
    "# Function to simulate human-like waiting (random sleep time)\n",
    "def human_sleep(min_sleep=2, max_sleep=5):\n",
    "    time.sleep(random.uniform(min_sleep, max_sleep))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a55a55",
   "metadata": {},
   "source": [
    "### Funda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91382143",
   "metadata": {},
   "source": [
    "TBD:\n",
    "- ~~Filters for different attributes as an input~~\n",
    "- cleanup kenmerken, choose only columns you want and extract the rest to columns. \n",
    "- ~~process listings on next page of homepage and so forth~~\n",
    "- setup orchestrator/trigger rule. \n",
    "-  ~~Have a comparision step before scrape. If url already in df, then ignore and go to next url listing.~~\n",
    "- ~~Extract phone number & makelaar name ~~\n",
    "- On the home/search_results page, check for status. Some of them i saw: 'Verhuurd onder voorbehoud', 'Onder optie'. Can be usueful to update existing scraped article status. but might be too much work. \n",
    "- Email/Telegram automatic message or something\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d866ec95",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Funda Scrape Funcs\n",
    "\n",
    "def get_value_preceding_text(soup, text_search:str, element: str = \"span\"):\n",
    "    # Find the span that contains text_search\n",
    "    wonen_span = soup.find(element, string=lambda t: t and text_search in t.lower())\n",
    "\n",
    "    # Get the previous sibling elementt\n",
    "    if wonen_span:\n",
    "        prev_span = wonen_span.find_previous_sibling(element)\n",
    "        if prev_span:\n",
    "            return(prev_span.text.strip())\n",
    "        \n",
    "def kenmerken_extract(sec_kenmerken): ## extract kenmerken table funda\n",
    "    kenmerken_names = [i.text.strip() for i in sec_kenmerken.find_all(\"dt\")]\n",
    "    kenmerken_output = dict()\n",
    "    for i in kenmerken_names:\n",
    "        empty_dt = sec_kenmerken.find(\"dt\", string=i)\n",
    "\n",
    "        # Check if the <dt> is found and then find the corresponding <dd>\n",
    "        if empty_dt:\n",
    "            dd_element = empty_dt.find_next_sibling('dd')\n",
    "            if dd_element:\n",
    "                # print(\"Found <dd> text:\", dd_element.text.strip())\n",
    "                kenmerken_output[i] = dd_element.text.strip()\n",
    "                # return dd_element.text.strip()\n",
    "            else:\n",
    "                print(\"No <dd> found after <dt> with empty text.\")\n",
    "        else:\n",
    "            print(\"No <dt> with empty text found.\")\n",
    "    print(f\"Num Kenmerken: {len(list(kenmerken_output.values()))}\")\n",
    "    return kenmerken_output\n",
    "\n",
    "#function to extract phone number and format them a bit.\n",
    "def format_number(number):\n",
    "    # If the number contains '31' and doesn't start with '+', format it as '+31'\n",
    "    number = re.sub(r\"[^\\d]\", \"\", number)\n",
    "    if number.startswith('31'): ##'31' in number and not\n",
    "        return '+' + re.sub(r\"[^\\d]\", \"\", number)\n",
    "    # If the number starts with '06', leave it as '06'\n",
    "    elif number.startswith('06'):\n",
    "        return re.sub(r\"[^\\d]\", \"\", number)\n",
    "    else:\n",
    "        print('ERROR: Weird phone number.')\n",
    "        return number\n",
    "        \n",
    "\n",
    "def retrieve_html(url:str, driver):\n",
    "    driver.get(url)\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c676f1",
   "metadata": {},
   "source": [
    "Funda Filters\n",
    "\n",
    "selected_area=[\"utrecht\",\"amsterdam\"]&price=\"1000-\"&object_type=[\"apartment\",\"parking\"]&publication_date=\"3\"&floor_area=\"30-\"&plot_area=\"30-\"&rooms=\"1-\"&bedrooms=\"1-\"&bathrooms=\"1-\"&rental_agreement=[\"indefinite_duration\",\"temporary_rent\"]&renting_condition=[\"furnished\",\"partially_furnished\",\"service_cost_included\",\"service_cost_excluded\"]&energy_label=[\"A%2B%2B%2B%2B\",\"G\"]&exterior_space_type=[\"balcony\",\"terrace\",\"garden\"]&construction_type=[\"newly_built\",\"resale\"]&construction_period=[\"unknown\",\"before_1906\",\"from_1931_to_1944\"]&open_house=[\"all\",\"coming_weekend\",\"today\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f40a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##only use double quotes for the str\n",
    "input_dict = dict()\n",
    "input_dict[\"selected_area\"] = [\"utrecht\", \"rotterdam\", \"ijsselstein\"]\n",
    "input_dict[\"price\"] = \"600-1400\"\n",
    "input_dict[\"object_type\"] = [\"house\", \"apartment\"] #[\"house\",\"apartment\",\"parking\",\"land\",\"storage_space\",\"storage\",\"berth\",\"substructure\",\"pitch\"]\n",
    "# input_dict[\"publication_date\"]= \"5\"\n",
    "input_dict['availability']=[\"available\",\"negotiations\"] #\"unavailable\"\n",
    "# input_dict[\"floor_area\"] = \"30-\"\n",
    "# input_dict[\"plot_area\"]=\"30-\"\n",
    "# input_dict[\"rooms\"]=\"1-\"\n",
    "# input_dict[\"bedrooms\"] = \"1-\"\n",
    "# input_dict[\"bathrooms\"]=\"1-\"\n",
    "# input_dict[\"rental_agreement\"] = [\"indefinite_duration\", \"temporary_rent\"]\n",
    "# input_dict[\"renting_condition\"] = [\"furnished\", \"partially_finished\", \"service_cost_included\", \"service_cost_excluded\"]\n",
    "# input_dict[\"construction_type\"] = [\"newly_built\",\"resale\"]\n",
    "# input_dict[\"open_house\"] = [\"all\",\"coming_weekend\",\"today\"]\n",
    "\n",
    "all_filters = list()\n",
    "for i in input_dict.keys():\n",
    "    if isinstance(input_dict[i], str):\n",
    "        all_filters.append(f'{i}=\"{input_dict[i]}\"')\n",
    "    elif isinstance(input_dict[i], list):\n",
    "        # Convert list items to strings with double quotes\n",
    "        quoted_items = [f'\"{item}\"' for item in input_dict[i]]\n",
    "        all_filters.append(f\"{i}=[{','.join(quoted_items)}]\")\n",
    "\n",
    "\n",
    "# URL of homepage\n",
    "homepage_url = \"https://www.funda.nl/zoeken/huur\"+\"?\"+\"&\".join(all_filters)\n",
    "print(homepage_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa06f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process flow to extract ads from funda\n",
    "driver.get(homepage_url)\n",
    "\n",
    "# Wait for the page to load properly and the decline button to be clickable\n",
    "try:\n",
    "    # Wait for the decline button to be clickable (ensure the page has loaded)\n",
    "    decline_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.XPATH, '//*[@id=\"didomi-notice-disagree-button\"]'))\n",
    "    )\n",
    "\n",
    "    # Click the decline button\n",
    "    decline_button.click()\n",
    "    human_sleep()  # Simulate delay after clicking the decline button\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error while clicking decline button: {e}\")\n",
    "\n",
    "# Wait for some more time to ensure page content has loaded before scraping\n",
    "human_sleep(3, 6)\n",
    "\n",
    "# Allow more time for page content to load dynamically\n",
    "html = driver.page_source\n",
    "soup_search_results = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# driver.quit()\n",
    "\n",
    "### all <a> tag relating to page numbers. \n",
    "pages = soup_search_results.find_all('a', href=re.compile(r'\\?page=\\d+'))\n",
    "total_pages = max([int(re.search(r'(\\d+)$', i['href']).group(1)) for i in pages]) ## from list of page numbers scrapted, pick max\n",
    "\n",
    "\n",
    "\n",
    "listing_urls = []\n",
    "# Parse the JSON-LD data\n",
    "\n",
    "for i in np.arange(1, total_pages + 1):\n",
    "    if i > 1:\n",
    "        human_sleep()\n",
    "        soup_search_results = retrieve_html(homepage_url + \"&search_result=\"+str(i), driver)\n",
    "\n",
    "    script_tag = soup_search_results.find('script', {'type': 'application/ld+json'}) ## on the homepage/listings page, find the element containing urls linking to all listing.\n",
    "    if script_tag:\n",
    "        json_data = json.loads(script_tag.string)\n",
    "        # Extract URLs from the itemListElement\n",
    "        listing_urls.extend([item['url'] for item in json_data['itemListElement']])\n",
    "        \n",
    "        # Print out the extracted URLs\n",
    "        for url in listing_urls:\n",
    "            print(url)\n",
    "    else:\n",
    "        print(\"No script tag with JSON-LD data found.\")\n",
    "\n",
    "\n",
    "\n",
    "## Check if df already saved, and if yes if we already scraped the listing. \n",
    "processing_listing_urls = []\n",
    "for url in listing_urls:\n",
    "    if os.path.exists(output_file):\n",
    "            saved_df = pd.read_parquet(output_file)\n",
    "            if url in saved_df['listing_url'].values:\n",
    "                print('Already scraped. Ignore or repeat scrape later.')\n",
    "            else:\n",
    "                print('New listing. Processing:')\n",
    "                processing_listing_urls.append(url)\n",
    "\n",
    "    else:\n",
    "        saved_df = pd.DataFrame()\n",
    "        processing_listing_urls.append(url)\n",
    "\n",
    "\n",
    "df_funda = pd.DataFrame(columns=['price', 'address', 'city', 'province', 'postcode', 'living_area', 'num_bedrooms', 'kenmerken', 'omschrijving', 'phone', 'makelaar_name', 'makelaar_url', 'status', 'listing_url', 'timestamp'])\n",
    "for url in processing_listing_urls:\n",
    "    output_list = [None] * len(df_funda.columns)\n",
    "    # URL of the page you want to scrape\n",
    "    # url = listing_urls[0]\n",
    "    driver.get(url)\n",
    "    scrape_time = time.time()\n",
    "    human_sleep(3, 6)\n",
    "\n",
    "    # Allow more time for page content to load dynamically\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n",
    "\n",
    "    # Find the <h2> element with text \"Omschrijving\" and find the next div with class 'listing-description-text'\n",
    "    h2_element = soup.find('h2', string=\"Omschrijving\")\n",
    "\n",
    "    # Check if we found the h2 element\n",
    "    if h2_element:\n",
    "        # Find the next div element after the h2\n",
    "        description_div = h2_element.find_next('div', class_='listing-description-text')\n",
    "        \n",
    "        # Extract and print the text inside the description div\n",
    "        if description_div:\n",
    "            omschrijving = description_div.get_text(strip=True)\n",
    "            print(omschrijving)\n",
    "        else:\n",
    "            print(\"Description div not found.\")\n",
    "    else:\n",
    "        print(\"Heading 'Omschrijving' not found.\")\n",
    "\n",
    "    ## extract price\n",
    "    ld_json_script = soup.find('script', type='application/ld+json')\n",
    "    price = json.loads(ld_json_script.string)['offers']['price']\n",
    "\n",
    "    output_list[0] = price \n",
    "\n",
    "    ## extract title of page (address)\n",
    "\n",
    "    title_text = soup.title.get_text()\n",
    "\n",
    "    # Step 2: Use regex to extract text after colon and before \"[Funda]\" (if present)\n",
    "    match = re.search(r':\\s*(.*?)\\s*(?:\\[Funda\\])?$', title_text)\n",
    "    if match:\n",
    "        address = match.group(1)\n",
    "        print('Address: '+ address)  # Output: Hoekeindseweg 162 2665 KH Bleiswijk\n",
    "    else:\n",
    "        print(\"No match found.\")\n",
    "    output_list[1] = address\n",
    "    # soup.find(class_=\"md:font-bold\") #surface area\n",
    "    city_divs = soup.find_all('div', attrs={\"city\": True}) ## contains all info about the house address\n",
    "    city_divs[0]['city']\n",
    "    city_divs[0]['province']\n",
    "    city_divs[0]['housenumber']\n",
    "    city_divs[0]['neighborhoodidentifier']\n",
    "    city_divs[0]['postcode']\n",
    "\n",
    "    output_list[2] = city_divs[0]['city']\n",
    "    output_list[3] = city_divs[0]['province']\n",
    "    output_list[4] = city_divs[0]['postcode']\n",
    "\n",
    "            \n",
    "    living_area = get_value_preceding_text(soup, 'wonen')\n",
    "    plot_size = get_value_preceding_text(soup, 'perceel')\n",
    "    slaapkamers = get_value_preceding_text(soup, 'slaapkamers')\n",
    "\n",
    "\n",
    "    # output_list[5] = living_area\n",
    "    output_list[6] = slaapkamers\n",
    "\n",
    "\n",
    "    sec_kenmerken = soup.find('section', {'id': 'features'})\n",
    "\n",
    "\n",
    "    alle_kenmerken = kenmerken_extract(sec_kenmerken)\n",
    "    print(alle_kenmerken)\n",
    "    output_list[7] = alle_kenmerken\n",
    "\n",
    "    output_list[5] = re.search(r'\\d+', alle_kenmerken['Wonen']).group() ## specify wonen/living_area\n",
    "\n",
    "    omschrijving = soup.find('h2', string=\"Omschrijving\").find_next('div', class_='listing-description-text').get_text(strip=True)\n",
    "    print(omschrijving)\n",
    "    output_list[8] = omschrijving\n",
    "\n",
    "    ### Extract phone number for listing\n",
    "    elements = soup.find_all(lambda tag: tag.name and tag.string and \"Bel\" in tag.string and any(char.isdigit() for char in tag.string)) ## look for word bel and numeric\n",
    "    try:\n",
    "        raw_text = elements[0].get_text()  # Get the text content from the element\n",
    "        # Extract the phone number part using regex\n",
    "        match = re.search(r\"(\\+?\\(?\\d+\\)?[\\d\\- ]+)\", raw_text)  # Match a phone number\n",
    "        if match:\n",
    "            phone_number = match.group(0)  # Extract the phone number from the match\n",
    "            formatted_phone = format_number(phone_number)  # Format the number\n",
    "            print(\"Formatted number:\", formatted_phone)\n",
    "        else:\n",
    "            print(\"No valid phone number found in:\", raw_text)\n",
    "    except Exception as e:\n",
    "        print(f'Phone number not found. {e}')\n",
    "        formatted_phone = None\n",
    "\n",
    "    output_list[9] = formatted_phone\n",
    "\n",
    "    ### Extract makelaar name and url\n",
    "    try:\n",
    "        for a_tag in soup.find_all('a', href=True):  # Find <a> tags with href attribute\n",
    "            href = a_tag['href']\n",
    "            if 'https://www.funda.nl/makelaars/' in href.lower():  # Check if 'makelaar' is in href\n",
    "                # Get the title attribute (if exists), default to empty string if not\n",
    "                title = a_tag.get('title', '')\n",
    "                makelaar_details = {'makelaar_url':href, 'makelaar_name':title}\n",
    "    except Exception as e:\n",
    "        print(f'Makelaar name not found. {e}')\n",
    "        makelaar_details = {'makelaar_url':None, 'makelaar_name':None}\n",
    "\n",
    "    output_list[10] = makelaar_details['makelaar_name']\n",
    "    output_list[11] = makelaar_details['makelaar_url']\n",
    "    output_list[-3] = alle_kenmerken['Status']\n",
    "    output_list[-2] = url\n",
    "    output_list[-1] = scrape_time\n",
    "    df_funda.loc[len(df_funda)] = output_list\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "## saving df and appending any new scraping\n",
    "updated_df = pd.concat([saved_df, df_funda], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame back to output.parquet\n",
    "updated_df.to_parquet('output.parquet', index=False)\n",
    "print(\"Updated data saved to output.parquet\")\n",
    "\n",
    "\n",
    "# # Find the span that contains \"wonen\"\n",
    "# wonen_span = soup.find('span', string=lambda t: t and 'perceel' in t.lower())\n",
    "\n",
    "# # Get the previous sibling span - Useful to find area\n",
    "# if wonen_span:\n",
    "#     prev_span = wonen_span.find_previous_sibling('span')\n",
    "#     if prev_span:\n",
    "#         print(prev_span.text.strip())  # Output: 110 mÂ²\n",
    "# soup.find('section', {'id': 'features'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c9bf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4a4aa",
   "metadata": {},
   "source": [
    "## Pararius\n",
    "\n",
    "Try retrieving the same attributes as funda to maintain db consistency. Otherwise adds nones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c9167",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
